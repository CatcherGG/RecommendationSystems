{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6N98jxu7Jdqv"
   },
   "source": [
    "# Question 5\n",
    "\n",
    "## Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5QuBIjcL7J6"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from numpy import save\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1584202133281,
     "user": {
      "displayName": "Guy Gonen",
      "photoUrl": "",
      "userId": "13001566563642212261"
     },
     "user_tz": -120
    },
    "id": "4M2hXABX-bCt",
    "outputId": "065bc559-08a6-40d7-8afd-66971e72dccf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./trainData.csv')\n",
    "test = pd.read_csv('./testData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1584202133282,
     "user": {
      "displayName": "Guy Gonen",
      "photoUrl": "",
      "userId": "13001566563642212261"
     },
     "user_tz": -120
    },
    "id": "pkhbSTMCDWGd",
    "outputId": "00843923-1ca7-478a-f4b0-8f824cea67e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>AEx2SYEUJmTxVVB18LlCwA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>VR6GpWIda3SfvPC-lg9H3w</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Small unassuming place that changes their menu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>CKC0-MOWMqoeWf6s-szl8g</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Lester's is located in a beautiful neighborhoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                 user_id             business_id  stars  \\\n",
       "0          0  bv2nCi5Qv5vroFiqKGopiw  AEx2SYEUJmTxVVB18LlCwA    5.0   \n",
       "1          1  bv2nCi5Qv5vroFiqKGopiw  VR6GpWIda3SfvPC-lg9H3w    5.0   \n",
       "2          2  bv2nCi5Qv5vroFiqKGopiw  CKC0-MOWMqoeWf6s-szl8g    5.0   \n",
       "\n",
       "                                                text  \n",
       "0  Super simple place but amazing nonetheless. It...  \n",
       "1  Small unassuming place that changes their menu...  \n",
       "2  Lester's is located in a beautiful neighborhoo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "We will be using two main features the text mode:\n",
    "1. Pre-trained sentiment analysis model.\n",
    "2. Pre-trained word embedding model.\n",
    "\n",
    "For sentiment analysis we've used Polyglot pre-trained model based on: <b>Building sentiment lexicons for all major languages.</b> By Chen, Yanqing and Skiena, Steven. (https://www.aclweb.org/anthology/P14-2063.pdf)\n",
    "\n",
    "For word embeddings we've used SpaCy <b>en_core_web_lg</b>: GloVe vectors trained on Common Crawl. (https://spacy.io/models/en#en_core_web_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 197703,
     "status": "ok",
     "timestamp": 1584202335225,
     "user": {
      "displayName": "Guy Gonen",
      "photoUrl": "",
      "userId": "13001566563642212261"
     },
     "user_tz": -120
    },
    "id": "BF7c3bEX6d6_",
    "outputId": "9a40c39e-ffde-4879-e2b8-5e7b8149f643"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "import spacy\n",
    "# !python -m spacy download en_core_web_lg\n",
    "import en_core_web_lg\n",
    "\n",
    "nlp_en = spacy.load('en')\n",
    "word_embeddings = en_core_web_lg.load()\n",
    "\n",
    "# !pip install polyglot\n",
    "# !pip install pycld2 pyicu morfessor\n",
    "# !sudo apt-get install python-numpy libicu-dev\n",
    "# !polyglot download sentiment2.en sentiment2.fr\n",
    "\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentitment analysis examlple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The schnitzel tastes good - Polarity value: 1.0\n",
      "Sentence: The schnitzel tastes bad - Polarity value: -1.0\n"
     ]
    }
   ],
   "source": [
    "good_sentence = 'The schnitzel tastes good'\n",
    "bad_sentence = 'The schnitzel tastes bad'\n",
    "print('Sentence: %s - Polarity value: %s' % (good_sentence, Text(good_sentence).polarity))\n",
    "print('Sentence: %s - Polarity value: %s' % (bad_sentence, Text(bad_sentence).polarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9114693 ,  0.7643397 , -0.13391006, -0.55225426,  2.9559462 ,\n",
       "       -0.57415223,  2.1061962 , -0.9522961 ,  1.5902431 ,  0.4614852 ,\n",
       "       -1.2936845 ,  0.7127428 ,  0.29019094,  1.2715446 , -0.7468778 ,\n",
       "       -0.0960201 , -2.0786645 , -1.1211485 , -0.999015  , -1.5393571 ,\n",
       "        1.8427663 ,  0.42068043, -1.7615972 , -0.32483393,  0.9325883 ,\n",
       "        0.75593686, -0.78313994, -1.3573792 ,  0.19013476,  0.31226802,\n",
       "       -0.45489317,  1.889648  , -1.2618005 , -0.7740066 , -0.01927185,\n",
       "       -1.3993042 ,  1.2173991 ,  0.13923384, -1.8764429 ,  0.6271461 ,\n",
       "        2.2260892 ,  0.21308854,  0.07156163, -1.2292256 ,  1.1435161 ,\n",
       "       -0.42475027,  0.79920256, -0.11738998, -0.47466668,  2.9813936 ,\n",
       "        0.85535884, -0.03741038, -0.9331693 , -2.1958532 , -2.4001408 ,\n",
       "       -0.13666493,  1.5210004 ,  0.13124412, -0.5608901 ,  0.39753455,\n",
       "        1.1663913 ,  2.189408  , -0.00373243, -0.981498  ,  2.3745198 ,\n",
       "        0.77494216,  0.5314827 , -2.9572294 , -1.9545283 , -0.6729264 ,\n",
       "       -0.30943748, -0.29588997, -0.9934288 ,  1.3840101 , -1.178341  ,\n",
       "       -0.85850614,  1.3289027 , -1.5610092 , -1.5099504 , -0.3444376 ,\n",
       "       -0.8047069 , -0.78557837,  2.134013  ,  0.4355116 , -0.4799738 ,\n",
       "        0.27743238, -0.6073736 ,  2.193396  , -0.102072  , -0.1571757 ,\n",
       "        0.9677434 ,  0.8870167 , -0.6342195 ,  0.47663257,  0.70423067,\n",
       "        1.6194658 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_en(good_sentence).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text will be processed like this:\n",
    "1. <b>Tokenization</b>.\n",
    "2. <b>Lemmatization</b>: The reason we use lemmatization and not stemming is because we NEED the words to be actual words when using pre-modeled word embeddings.\n",
    "3. We are NOT removing stop words. It's not effective on sentiment analysis because some of the stop words are words like 'no' and 'not' which change the whole sentence meaning when they appear.\n",
    "4. <b>Sentitment analysis</b> - We will get polarity score between -1 (Bad) and 1 (Good) where 0 should be natural.\n",
    "5. <b>Word embeddings</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1584213343218,
     "user": {
      "displayName": "Guy Gonen",
      "photoUrl": "",
      "userId": "13001566563642212261"
     },
     "user_tz": -120
    },
    "id": "VnEKNue06gJM",
    "outputId": "8978adcd-ee26-490c-e36f-a7a14672ee26"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import TAG_MAP \n",
    "\n",
    "def get_sentence_with_lemmas(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        return nlp_en(' '.join(word.lemma_ for word in nlp_en(sentence)))\n",
    "    if isinstance(sentence, float):\n",
    "        get_sentence_with_lemmas(str(sentence))\n",
    "    return nlp_en(' '.join(word.lemma_ for word in sentence))\n",
    "\n",
    "def get_sentence_vector(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        return word_embeddings(sentence).vector\n",
    "    return word_embeddings(str(sentence)).vector\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    return nlp_en(''.join(word.string for word in sentence if not word.is_stop))\n",
    "\n",
    "def get_sentitment_analysis_polarity(sentence):\n",
    "    try:\n",
    "        return np.array([Text(sentence.replace('\\n', '')).polarity])\n",
    "    except:\n",
    "    # print(sentence)\n",
    "    # text = Text(sentence)\n",
    "    # print(\"{:<16}{}\".format(\"Word\", \"Polarity\")+\"\\n\"+\"-\"*30)\n",
    "    # for w in text.words:\n",
    "    #     print(\"{:<16}{:>2}\".format(w, w.polarity))\n",
    "        return np.zeros((1))\n",
    "\n",
    "def save_numpy(np_array, name):\n",
    "    from numpy import save\n",
    "    save(name+'.npy', np_array)\n",
    "    \n",
    "def load_numpy(name):\n",
    "    from numpy import load\n",
    "    return load(name+'.npy', allow_pickle=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemma_sentence = get_sentence_with_lemmas(text)\n",
    "    # no_stop_words_sentence = get_sentence_with_lemmas(lemma_sentence)\n",
    "    sentiment_analysis = get_sentitment_analysis_polarity(text)\n",
    "    return np.concatenate((get_sentence_vector(lemma_sentence), sentiment_analysis))\n",
    "\n",
    "assert np.array_equal(get_sentence_vector(nlp_en('The waiter was quick and polite.')), get_sentence_vector('The waiter was quick and polite.'))\n",
    "assert 'the waiter be quick and polite .' == str(get_sentence_with_lemmas('The waiter was quick and polite.'))\n",
    "assert 'waiter quick polite.' == str(remove_stop_words(nlp_en('The waiter was quick and polite.')))\n",
    "assert np.ones((1)) == get_sentitment_analysis_polarity('very good')\n",
    "\n",
    "len(list(TAG_MAP.keys()))\n",
    "\n",
    "def df_apply(df):\n",
    "    return df.apply(lambda text: preprocess_text(text))\n",
    "\n",
    "LIMIT = 250000\n",
    "def parallelize_dataframe(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    print('Starting up the engine.')\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of memory limitations we had to run feature extraction batches sized 250k.\n",
    "Then we've saved those array into files. \n",
    "\n",
    "The arrays were sized 301: 300 [Word embedding] and 1 [Sentitmenet analysis]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1iJ91Slvora8"
   },
   "outputs": [],
   "source": [
    "# train_word_embedding = parallelize_dataframe(train['text'][: LIMIT], df_apply)\n",
    "# save_numpy(train_word_embedding, 'train_word_embeddings')\n",
    "\n",
    "# train_word_embedding = parallelize_dataframe(train['text'][LIMIT : LIMIT * 2], df_apply)\n",
    "# save_numpy(train_word_embedding, 'train_word_embeddings_500k')\n",
    "\n",
    "# train_word_embedding = parallelize_dataframe(train['text'][LIMIT * 2 : LIMIT * 3], df_apply)\n",
    "# save_numpy(train_word_embedding, 'train_word_embeddings_750k')\n",
    "\n",
    "# train_word_embedding = parallelize_dataframe(test['text'][: LIMIT], df_apply)\n",
    "# save_numpy(test_word_embeddings, 'test_word_embeddings')\n",
    "\n",
    "# train_word_embedding = parallelize_dataframe(test['text'][LIMIT: LIMIT * 2], df_apply)\n",
    "# save_numpy(test_word_embeddings, 'test_word_embeddings_500k')\n",
    "\n",
    "# train_word_embedding = parallelize_dataframe(test['text'][LIMIT *2 : LIMIT * 3], df_apply)\n",
    "# save_numpy(test_word_embeddings, 'test_word_embeddings_750k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_numpy('./train_word_embeddings')\n",
    "y_train = train['stars'][:len(X_train)]\n",
    "\n",
    "X_test = load_numpy('./test_word_embeddings')\n",
    "y_test = train['stars'][:len(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1683,
     "status": "ok",
     "timestamp": 1584213371765,
     "user": {
      "displayName": "Guy Gonen",
      "photoUrl": "",
      "userId": "13001566563642212261"
     },
     "user_tz": -120
    },
    "id": "mxkmJ7Ktlc48",
    "outputId": "b48fefc0-a954-4171-88fd-894b8f679340"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', n_jobs=-1, max_iter=100, random_state=42).fit(np.stack(X_train), y_train.values)\n",
    "pred = clf.predict(np.stack(X_test))\n",
    "scores = cross_val_score(clf, np.stack(X_train), y_train.values, cv=5)\n",
    "\n",
    "def RMSE(pred, actual):\n",
    "  squared_error = np.sum((pred - actual) **2)\n",
    "  mean_squared_error = squared_error / actual.size\n",
    "  return pow(mean_squared_error, 0.5)\n",
    "\n",
    "def accuracy(pred, actual):\n",
    "  return np.sum(pred == actual) / actual.size\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test.values, pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['1', '2', '3', '4', '5'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "print('RMSE:', RMSE(pred, y_test.values))\n",
    "print('Accuracy:', accuracy(pred, y_test.values))\n",
    "print('Cross validation scores:', scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPzSMgX5RKfRm2twFxtojYR",
   "collapsed_sections": [],
   "mount_file_id": "10jHTa8w_4YXDyxlzxbbXaimEO-26D80G",
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
